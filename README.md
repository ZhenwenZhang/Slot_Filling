# Research Advances In Semantic Slot Filling 

This repo mainly summary latest research advances on semantic slot filling.

# Performance
| Methods | F1 | Published Address |Paper|
| ------ | ------ |------ |------------ |
| Simple RNN | 0.9411 |Interspeech, 2013|Link|
| CNN-CRF | 0.9435 | ASRU,2013 |Link|
|  LSTM | 0.9485 |SLT Workshop,2014 |Link|
| RNN-SOP | 0.9489 |NIPS Workshop,2015 |Link|
| Deep LSTM | 0.9508  | SLT Workshop,2014|Link|
| RNN-EM | 0.9525 |arXiv,2015 |Link|
| Bi-RNN with ranking loss | 0.9547| 2018|Link|
| Sequential CNN | 0.9561|2018 |Link|
| Encoder-labeler Deep LSTM | 0.9566| das|Link|
| BiLSTM-LSTM (focus) | 0.9579| 2018|Link|
| Neural Sequence Chunking | 0.9586 |2018 |Link|
# Related Papers



 [Peng, Baolin, and Kaisheng Yao. "Recurrent neural networks with external memory for language understanding." arXiv preprint arXiv:1506.00195 (2015).](https://arxiv.org/abs/1506.00195.pdf)

1.[Peng, B., and Yao, K. 2015. Recurrent neural networks with external memory for language understanding.arXiv.](https://ieeexplore.ieee.org/abstract/document/7078572)


1.[Liu, B., and Lane, I. 2015. Recurrent neural network structured output prediction for spoken language understanding. In NIPS Workshop.](https://pdfs.semanticscholar.org/b75b/59f38c874a920102834c9e218c960fc35c81.pdf)


1.[Yao, Kaisheng, et al. "Spoken language understanding using long short-term memory neural networks." Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014.](https://groups.csail.mit.edu/sls/publications/2014/Zhang_SLT_2014.pdf)

1.["Recurrent neural networks for language understanding."](https://www.isca-speech.org/archive/archive_papers/interspeech_2013/i13_2524.pdf)

2.[ Mesnil, Grégoire, et al. "Using recurrent neural networks for slot filling in spoken language understanding." IEEE/ACM Transactions on Audio, Speech, and Language Processing 2015](https://ieeexplore.ieee.org/abstract/document/6998838)

3.[ Kurata, Gakuto, et al. "Leveraging sentence-level information with encoder lstm for semantic slot filling." EMNLP 2016](https://arxiv.org/abs/1601.01530.pdf)

4.[ Hakkani-Tür, Dilek, et al. "Multi-Domain Joint Semantic Frame Parsing Using Bi-Directional RNN-LSTM." Interspeech 2016](https://pdfs.semanticscholar.org/d644/ae996755c803e067899bdd5ea52498d7091d.pdf)

5.[ Zhang, Xiaodong, and Houfeng Wang. "A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding." IJCAI 2016](https://www.ijcai.org/Proceedings/16/Papers/425.pdf)

6.[ Liu, Bing, and Ian Lane. "Attention-based recurrent neural network models for joint intent detection and slot filling." Interspeech 2016](https://arxiv.org/abs/1609.01454)

7.[ Constantin, Stefan, Jan Niehues, and Alex Waibel. "Multi-task learning to improve natural language understanding." 2018](https://arxiv.org/abs/1812.06876.pdf)

8.[ Zhao, Lin, and Zhe Feng. "Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention." ACL 2018](http://www.aclweb.org/anthology/P18-2068)

9.[ Gong, Yu, et al. "Deep Cascade Multi-task Learning for Slot Filling in Online Shopping Assistant." 2019](http://www.cs.sjtu.edu.cn/~kzhu/papers/kzhu-slot.pdf)
